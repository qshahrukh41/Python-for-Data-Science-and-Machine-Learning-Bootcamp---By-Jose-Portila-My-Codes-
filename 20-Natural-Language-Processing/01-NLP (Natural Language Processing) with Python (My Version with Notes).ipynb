{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP (Natural Language Processing) with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the Corpus for Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#downloading the corpus for stopwords\n",
    "#Type l to browse the list. hit enter untill you find stopwords corpus\n",
    "#Hit enter untill the list ends\n",
    "#Then press 'd' and then type 'stopwords' to download the package\n",
    "#Then type 'q' and enter to quit the shell\n",
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UCI SMS Spam Collection Data Set\n",
    "#You can checkout the readme file there to get more info\n",
    "#There are 5000 text messages in the dataset\n",
    "#Folder location of dataset 'smsspamcollection/SMSSpamCollection'\n",
    "#Let's go ahead and use rstrip() plus a list comprehension to get a list of all the lines of text messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [line.rstrip() for line in open('smsspamcollection/SMSSpamCollection')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574\n"
     ]
    }
   ],
   "source": [
    "# 5574 text messages\n",
    "print(len(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham\\tWhat you thinked about me. First time you saw me in class.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# messages[index] to see an actual message\n",
    "#messages[0]\n",
    "messages[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of texts is also sometimes called \"corpus\". Let's print the first ten messages and number them using **enumerate**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "\n",
      "\n",
      "1 ham\tOk lar... Joking wif u oni...\n",
      "\n",
      "\n",
      "2 spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "\n",
      "\n",
      "3 ham\tU dun say so early hor... U c already then say...\n",
      "\n",
      "\n",
      "4 ham\tNah I don't think he goes to usf, he lives around here though\n",
      "\n",
      "\n",
      "5 spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\n",
      "\n",
      "\n",
      "6 ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
      "\n",
      "\n",
      "7 ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
      "\n",
      "\n",
      "8 spam\tWINNER!! As a valued network customer you have been selected to receivea Â£900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
      "\n",
      "\n",
      "9 spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#To look at first 10 messages\n",
    "#If we look carefully, each message is labeled as ham or spam and\n",
    "#then after a tab seperation the actual message\n",
    "for mess_no,message in enumerate(messages[:10]):\n",
    "    print(mess_no,message)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's convert the data into pandas datframe using tab seperation\n",
    "messages = pd.read_csv('smsspamcollection/SMSSpamCollection',sep='\\t',names=['label','message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's check out some of the stats with some plots and the built-in methods in pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.groupby('label').describe()\n",
    "#Wee see there are 4825 ham messages and 747 spam messages\n",
    "#Also the top spam and ham message and their frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "As we continue our analysis we want to start thinking about the features we are going to be using. This goes along with the general idea of [feature engineering](https://en.wikipedia.org/wiki/Feature_engineering). The better your domain knowledge on the data, the better your ability to engineer more features from it. Feature engineering is a very large part of spam detection in general. I encourage you to read up on the topic!\n",
    "\n",
    "Let's make a new column to detect how long the text messages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large part of NLP will be feature engineering\n",
    "# The better you have domain knowledge, the better you can extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's figure out the length of each messages\n",
    "messages['length'] = messages['message'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "Let's visualize this! Let's do the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets visuallize the length of the messages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x196178f4d88>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "messages['length'].plot.hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.489950\n",
       "std        59.942907\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Max length of a message is 910\n",
    "messages['length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[messages['length'] == 910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000001961ACB5A88>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000001961ACE4D88>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# facetgrid histogram using pandas data vis\n",
    "# A very important thing we get here is that,\n",
    "# length of spam messages is usually much larger than ham messages\n",
    "# Thus, length is a big feature\n",
    "messages.hist(column='length', by='label', bins=60,figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert a corpus of string to a vector format. \n",
    "# Simplest approach is bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mess = 'Sample message! Notice: it has punctuation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see the list of punctuations\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing all the punctuations in mess\n",
    "nopunc = [c for c in mess if c not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S',\n",
       " 'a',\n",
       " 'm',\n",
       " 'p',\n",
       " 'l',\n",
       " 'e',\n",
       " ' ',\n",
       " 'm',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'a',\n",
       " 'g',\n",
       " 'e',\n",
       " ' ',\n",
       " 'N',\n",
       " 'o',\n",
       " 't',\n",
       " 'i',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 't',\n",
       " ' ',\n",
       " 'h',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already downloaded the stopwords library. Now let's import\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To see the list of common english stopwords list\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add the list of letters excluding punctuations\n",
    "nopunc = ''.join(nopunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample message Notice it has punctuation'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we have our message without any punctuation\n",
    "nopunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's see how join function works\n",
    "x = ['a','b','c','d']\n",
    "'*'.join(x) #join the list of items with a '*' between them\n",
    "'#'.join(x) #join the list of items with a '#' between them\n",
    "''.join(x)  #Simply join the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'it', 'has', 'punctuation']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nopunc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's remove the stopwords using list comprehension and stopwords library\n",
    "clean_mess = [word for word in nopunc.split() if word.lower() not in stopwords.words('English')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message', 'Notice', 'punctuation']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopwords removed\n",
    "clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets put everything together under a function\n",
    "def text_process(mess):\n",
    "    '''\n",
    "    1. remove punc\n",
    "    2. remove stopwords\n",
    "    3. return list of clean text words\n",
    "    '''\n",
    "    \n",
    "    nopunc = [c for c in mess if c not in string.punctuation]\n",
    "    \n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('English')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenization: What we have done till now in our text_process function\n",
    "# This is an example of very basic text processing. We can also do text stemming and\n",
    "# lot of other things using the nltk library\n",
    "# Stemming is not going to do much help in this dataset because here a lot of shorthand is used. Such as: U, dun. hor, c, wif etc.\n",
    "\n",
    "messages['message'].head(5).apply(text_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing Normalization\n",
    "\n",
    "There are a lot of ways to continue normalizing this text. Such as [Stemming](https://en.wikipedia.org/wiki/Stemming) or distinguishing by [part of speech](http://www.nltk.org/book/ch05.html).\n",
    "\n",
    "NLTK has lots of built-in tools and great documentation on a lot of these methods. Sometimes they don't work well for text-messages due to the way a lot of people tend to use abbreviations or shorthand, For example:\n",
    "    \n",
    "    'Nah dawg, IDK! Wut time u headin to da club?'\n",
    "    \n",
    "versus\n",
    "\n",
    "    'No dog, I don't know! What time are you heading to the club?'\n",
    "    \n",
    "Some text normalization methods will have trouble with this type of shorthand and so I'll leave you to explore those more advanced methods through the [NLTK book online](http://www.nltk.org/book/).\n",
    "\n",
    "For now we will just focus on using what we have to convert our list of words to an actual vector that SciKit-Learn can use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorization/Frequency Count\n",
    "<table border = “1“>\n",
    "<tr>\n",
    "<th></th> <th>Message 1</th> <th>Message 2</th> <th>...</th> <th>Message N</th> \n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 1 Count</b></td><td>0</td><td>1</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word 2 Count</b></td><td>0</td><td>0</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>...</b></td> <td>1</td><td>2</td><td>...</td><td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><b>Word N Count</b></td> <td>0</td><td>1</td><td>...</td><td>1</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we figure out frequency. That is count how many times does a word occur in each message. To  do that,\n",
    "# We need a matrix where row consists of all the unique words in the dataset and columns consist of all the messages\n",
    "# value of each cell in the matrix represents, frequency of a word(row) for the corresponding message(column)\n",
    "# There will be a lot of zeros in the matrix and the matrix size will be huge.\n",
    "# So scikit learn uses, Sparse matrix to save memory. Learn more about sparse matrix from wiki\n",
    "# Sparse matrix has a lot more zero elements than non-zero elements.\n",
    "# It is used for storage and computing time\n",
    "# It avoids zero elements and only evaluates the non-zero elements, thus less memory\n",
    "# Computing time also gets saved by logically designing  a data structure that only traverses non-zero elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer takes a lot of parameters. \n",
    "#We'll simply pass our buil_in function text_process to it. \n",
    "#and now we'll fit it to messages['message']. \n",
    "#This will take a while since it is a huge matrix\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(messages['message'])\n",
    "#bow stands for bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11425\n"
     ]
    }
   ],
   "source": [
    "#To see total number of vocabulary words in our 2D matrix\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets take message number 4 from our dataset\n",
    "mess4 = messages['message'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U dun say so early hor... U c already then say...'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mess4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets see how bow_transformer converts the message\n",
    "bow4 = bow_transformer.transform([mess4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4068)\t2\n",
      "  (0, 4629)\t1\n",
      "  (0, 5261)\t1\n",
      "  (0, 6204)\t1\n",
      "  (0, 6222)\t1\n",
      "  (0, 7186)\t1\n",
      "  (0, 9554)\t2\n"
     ]
    }
   ],
   "source": [
    "#We see 7 words in message4. Two of them occured twice and the rest only once\n",
    "print(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11425)\n"
     ]
    }
   ],
   "source": [
    "#shape of bow4. 1x11425. here 11425 is the list of unique words in the dataset\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To see the word at 4068 and 9554\n",
    "bow_transformer.get_feature_names()[4068]\n",
    "bow_transformer.get_feature_names()[9554]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets convert the entire dataset to a sparse matrix using bag of words transformer\n",
    "messages_bow = bow_transformer.transform(messages['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix: (5572, 11425)\n"
     ]
    }
   ],
   "source": [
    "#To check the shape of the sparse matrix\n",
    "print('Shape of Sparse Matrix:', messages_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50548"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To check the amount of non-zero elements in the sparse matrix\n",
    "#50,548 occurances of non-zero elements\n",
    "messages_bow.nnz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.07940295412668218\n"
     ]
    }
   ],
   "source": [
    "#Sparsity of the sparse matrix: (number of non-zero elements)/ (Total no of elements in the matrix)\n",
    "#Total no of elements in the matrix = no of rows * no of cols\n",
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF using scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to get an idea what it looks like we'll tf-idf transform message 4\n",
    "tfidf4 = tfidf_transformer.transform(bow4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9554)\t0.5385626262927564\n",
      "  (0, 7186)\t0.4389365653379857\n",
      "  (0, 6222)\t0.3187216892949149\n",
      "  (0, 6204)\t0.29953799723697416\n",
      "  (0, 5261)\t0.29729957405868723\n",
      "  (0, 4629)\t0.26619801906087187\n",
      "  (0, 4068)\t0.40832589933384067\n"
     ]
    }
   ],
   "source": [
    "#we have converted the word count to term frequency to inverse document frequency\n",
    "#Basically we have weight values for each of these words with respect to the actual document\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.527076498901426"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets say, we want to check tfidf value for a particular word. For instance: 'university'\n",
    "tfidf_transformer.idf_[bow_transformer.vocabulary_['university']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll tf_idf our whole dataset matrix\n",
    "messages_tfidf = tfidf_transformer.transform(messages_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There are many ways of data preprocessing.\n",
    "#These include feature engineering and pipeline.\n",
    "#You can check out scikit learns official documentation on dealing with text data\n",
    "#Also you can check available papers and books on the general topic of nlp\n",
    "#This is a really large field and lots of resources are there\n",
    "#For a variety of reasons, naive bayes classifier works good with text data\n",
    "#That's why we'll be using it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We'll be using Multinomial Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(messages_tfidf,messages['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham'], dtype='<U4')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We check that the foruth message is detected as ham\n",
    "spam_detect_model.predict(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#And the actual label of fourth message is also ham.\n",
    "messages['label'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict for the whole dataset\n",
    "all_pred = spam_detect_model.predict(messages_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'spam', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But we have used training data as the whole dataset. But it is not actually correct\n",
    "#Let's split the data to train and test set to see the true power of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_train, msg_test, label_train, label_test = train_test_split(messages['message'], messages['label'], test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2764    Say this slowly.? GOD,I LOVE YOU &amp; I NEED ...\n",
       "90      Yeah do! Don‘t stand to close tho- you‘ll catc...\n",
       "4754    Cashbin.co.uk (Get lots of cash this weekend!)...\n",
       "721                         Oh is it? Send me the address\n",
       "1904    Free entry in 2 a weekly comp for a chance to ...\n",
       "                              ...                        \n",
       "2717    House-Maid is the murderer, coz the man was mu...\n",
       "2449                         Do u knw dis no. &lt;#&gt; ?\n",
       "1832    Hello- thanx for taking that call. I got a job...\n",
       "79      Its not the same here. Still looking for a job...\n",
       "245                 U don't remember that old commercial?\n",
       "Name: message, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see that msg_train now has unpreprocessed training data\n",
    "#That means we have to again repeat the preprocessing step onto our training set\n",
    "#But fortunately, sklearn has way of saving our workflow pipeline so that we dont have to do this all over again\n",
    "#Let's see how we can do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have to define here the list of steps we require to do\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=text_process)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier',MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x000001961AF0B048>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can fit our train data to the pipeline and it will automatically preprocess our data\n",
    "#It will take some time as there are a lot of steps\n",
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_process at 0x000001961AF0B048>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we'll just have to fit our training data to the model\n",
    "pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we'll predict the labels of test data\n",
    "predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.98      1441\n",
      "        spam       1.00      0.70      0.82       231\n",
      "\n",
      "    accuracy                           0.96      1672\n",
      "   macro avg       0.98      0.85      0.90      1672\n",
      "weighted avg       0.96      0.96      0.95      1672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Now let's see our classification_report\n",
    "print(classification_report(label_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can change the classifier. For instance:\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#and then set RandomForestClassifier as your classification algorithm and check the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP is huge subfield of ML. Thus, you need to do a lot of practice and explore resources to get a hold of this field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
